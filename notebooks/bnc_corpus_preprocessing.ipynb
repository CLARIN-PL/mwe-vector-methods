{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import BigramCollocationFinder, word_tokenize, pos_tag\n",
    "from nltk.corpus.reader import BNCCorpusReader\n",
    "from nltk.collocations import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/netherwulf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download perceptron PoS tagger\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read British National Corpus\n",
    "\n",
    "bnc_texts_dir = os.path.join('..', 'storage', 'bnc', 'raw_data', 'BNC', 'Texts')\n",
    "\n",
    "bnc_reader = BNCCorpusReader(root=bnc_texts_dir, fileids=r'[A-K]/\\w*/\\w*\\.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bigram association measures\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "measure_dict = {'pmi' : bigram_measures.pmi,\n",
    "                'dice' : bigram_measures.dice,\n",
    "                'chi2' : bigram_measures.chi_sq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize finder and preprocess BNC corpus\n",
    "\n",
    "# treat each sentence as another document (find MWEs inside sentences)\n",
    "finder = BigramCollocationFinder.from_documents(bnc_reader.sents())\n",
    "\n",
    "# treat whole corpus as a long sentence (find MWEs inside and BETWEEN sentences)\n",
    "# finder = BigramCollocationFinder.from_words(bnc_reader.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curr_time():\n",
    "    return f'{datetime.datetime.now().strftime(\"%H:%M:%S\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(nltk_pos_tag):\n",
    "    if nltk_pos_tag[0] == 'J':\n",
    "        return 'ADJ'\n",
    "        \n",
    "    if nltk_pos_tag[0] == 'N':\n",
    "        return 'NOUN'\n",
    "\n",
    "    if nltk_pos_tag[0] == 'V':\n",
    "        return 'VERB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check word correctness\n",
    "def check_word_correctness(word) -> bool:\n",
    "    if len(word) < 1:\n",
    "        return False\n",
    "    # print(f'word: {word}',\n",
    "    #       f'pos_tag: {pos_tag([word])}',\n",
    "    #       f'pos_tag([word])[0][1][0]: {pos_tag([word])[0][1][0]}',\n",
    "    #       sep='\\n')\n",
    "\n",
    "    if any(char in string.punctuation for char in word):\n",
    "        return False\n",
    "\n",
    "    if any(char in ['`', '~', '‘', '—', '\\'', '’'] for char in word):\n",
    "        return False\n",
    "\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return False\n",
    "\n",
    "    if word[0].isupper():\n",
    "        return False\n",
    "\n",
    "    if pos_tag([word])[0][1][0] not in ['J', 'N', 'V']:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of occurrences for MWE\n",
    "def get_mwe_freq(mwe, freq_list):\n",
    "    # mwe_tuple = [mwe_tuple for mwe_tuple in freq_list if mwe_tuple[0] == mwe]\n",
    "    return freq_list[mwe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of measures to the TSV file\n",
    "def save_mwe_list(mwe_list, measure_name, dataset_name, dataset_dir):\n",
    "    out_filepath = os.path.join(dataset_dir, f'{dataset_name}_{measure_name}_incorrect_mwe.tsv')\n",
    "\n",
    "    with open(out_filepath, 'w') as out_file:\n",
    "        out_file.write('\\t'.join(['first_word_tag', 'second_word_tag', 'first_word', 'second_word', 'measure_value', 'frequency']) + '\\n')\n",
    "\n",
    "        for mwe_tuple in mwe_list:\n",
    "            out_file.write('\\t'.join(mwe_tuple) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:20:20 : Generating incorrect MWEs list for pmi\n"
     ]
    }
   ],
   "source": [
    "# get cleaned list of MWEs for list of measures\n",
    "\n",
    "dataset_name = 'bnc'\n",
    "dataset_dir = os.path.join('..', 'storage', dataset_name, 'preprocessed_data')\n",
    "\n",
    "for measure_name in measure_dict.keys():\n",
    "    if measure_name != 'pmi':\n",
    "        continue\n",
    "\n",
    "    print(f'{get_curr_time()} : Generating incorrect MWEs list for {measure_name}')\n",
    "\n",
    "    # get list of MWEs with spoecified measure\n",
    "    desc_mwe_list = finder.score_ngrams(measure_dict[measure_name])\n",
    "\n",
    "    # get frequencies of MWE\n",
    "    freq_mwe_list = {k: v for k, v in finder.ngram_fd.items()}\n",
    "\n",
    "    # clean list of MWEs\n",
    "    # asc_mwe_list_cleaned = [mwe_tuple for mwe_tuple in desc_mwe_list[::-1] if all([check_word_correctness(mwe_word) for mwe_word in mwe_tuple[0]]) and mwe_tuple[1] > 1]\n",
    "    asc_mwe_list_cleaned = [mwe_tuple for mwe_tuple in desc_mwe_list[::-1] if all([check_word_correctness(mwe_word) for mwe_word in mwe_tuple[0]])]\n",
    "\n",
    "    # get last values from the end of the 3rd quartile\n",
    "    asc_mwe_list_cleaned = asc_mwe_list_cleaned[int(0.75 * len(asc_mwe_list_cleaned)) - 100000:int(0.75 * len(asc_mwe_list_cleaned))]\n",
    "\n",
    "    # get list with MWE, measure value and frequency\n",
    "    mwe_with_freq = [[get_pos_tag(pos_tag([mwe_tuple[0][0]])[0][1]), get_pos_tag(pos_tag([mwe_tuple[0][1]])[0][1]), mwe_tuple[0][0], mwe_tuple[0][1], str(mwe_tuple[1]), str(get_mwe_freq(mwe_tuple[0], freq_mwe_list))] for mwe_tuple in asc_mwe_list_cleaned]\n",
    "\n",
    "    # save dataset\n",
    "    save_mwe_list(mwe_with_freq, measure_name, dataset_name, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:47:36 : Generating incorrect MWEs list for pmi\n"
     ]
    }
   ],
   "source": [
    "# get cleaned list of MWEs for single measure\n",
    "\n",
    "measure_name = 'pmi'\n",
    "\n",
    "dataset_name = 'bnc'\n",
    "\n",
    "dataset_dir = os.path.join('..', 'storage', dataset_name, 'preprocessed_data')\n",
    "\n",
    "print(f'{get_curr_time()} : Generating incorrect MWEs list for {measure_name}')\n",
    "\n",
    "# get list of MWEs with spoecified measure\n",
    "desc_mwe_list = finder.score_ngrams(measure_dict[measure_name])\n",
    "\n",
    "# get frequencies of MWE\n",
    "freq_mwe_list = {k: v for k, v in finder.ngram_fd.items()}\n",
    "\n",
    "# clean list of MWEs\n",
    "asc_mwe_list_cleaned = [mwe_tuple for mwe_tuple in desc_mwe_list[::-1] if all([check_word_correctness(mwe_word) for mwe_word in mwe_tuple[0]]) and mwe_tuple[1] > 1]\n",
    "# asc_mwe_list_cleaned = [mwe_tuple for mwe_tuple in desc_mwe_list[::-1] if all([check_word_correctness(mwe_word) for mwe_word in mwe_tuple[0]])]\n",
    "\n",
    "# get only first 100,000 MWEs with lowest measure value\n",
    "asc_mwe_list_cleaned = asc_mwe_list_cleaned[:100000]\n",
    "\n",
    "# get last values from the end of the 3rd quartile\n",
    "# asc_mwe_list_cleaned = asc_mwe_list_cleaned[int(0.75 * len(asc_mwe_list_cleaned)) - 100000:int(0.75 * len(asc_mwe_list_cleaned))]\n",
    "\n",
    "# get list with MWE, measure value and frequency\n",
    "mwe_with_freq = [[get_pos_tag(pos_tag([mwe_tuple[0][0]])[0][1]), get_pos_tag(pos_tag([mwe_tuple[0][1]])[0][1]), mwe_tuple[0][0], mwe_tuple[0][1], str(mwe_tuple[1]), str(get_mwe_freq(mwe_tuple[0], freq_mwe_list))] for mwe_tuple in asc_mwe_list_cleaned]\n",
    "\n",
    "# save dataset\n",
    "save_mwe_list(mwe_with_freq, f'{measure_name}_greater_than_1', dataset_name, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentence_df(sentence_df, filepath):\n",
    "    with open(filepath, 'w', encoding='utf8') as f_out:\n",
    "        for row_idx in range(len(sentence_df)):\n",
    "            f_out.write('\\t'.join(sentence_df.loc[row_idx, :].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences containing MWEs from the list\n",
    "\n",
    "# load MWE lists\n",
    "incorr_lists_dir = os.path.join('..', 'storage', 'bnc', 'preprocessed_data')\n",
    "\n",
    "pmi_incorr_list_filename = 'bnc_pmi_greater_than_1_incorrect_mwe.tsv'\n",
    "dice_incorr_list_filename = 'bnc_dice_end_of_3rd_quatile_incorrect_mwe.tsv'\n",
    "chi2_incorr_list_filename = 'bnc_chi2_end_of_3rd_quartile_incorrect_mwe.tsv'\n",
    "\n",
    "incorr_list_path = os.path.join(incorr_lists_dir, pmi_incorr_list_filename)\n",
    "\n",
    "incorr_mwe_df = pd.read_csv(incorr_list_path, sep = '\\t')\n",
    "\n",
    "incorr_mwe_list = [[str(first_word), str(second_word)] for first_word, second_word in zip(incorr_mwe_df['first_word'].tolist(), \n",
    "                                                                                          incorr_mwe_df['second_word'].tolist()) if first_word != '’' and second_word != '’']\n",
    "sent_df = pd.DataFrame(columns = ['first_word_tag', 'second_word_tag',\n",
    "                                  'first_word', 'second_word', \n",
    "                                  'first_word_id', 'second_word_id', \n",
    "                                  'sentence'])\n",
    "sent_idx = 0\n",
    "for sentence in bnc_reader.sents():\n",
    "\n",
    "    for mwe_ind, incorr_mwe in enumerate(incorr_mwe_list):\n",
    "        \n",
    "        if incorr_mwe[0] in sentence:\n",
    "            first_word_id = sentence.index(incorr_mwe[0])\n",
    "\n",
    "            if first_word_id < (len(sentence) - 1) and sentence[first_word_id + 1] == incorr_mwe[1]:\n",
    "\n",
    "                mwe_row = incorr_mwe_df.iloc[[mwe_ind]].values.tolist()[0]\n",
    "\n",
    "                sent_df = sent_df.append({'first_word_tag': mwe_row[0],\n",
    "                                          'second_word_tag': mwe_row[1],\n",
    "                                          'first_word': mwe_row[2],\n",
    "                                          'second_word': mwe_row[3],\n",
    "                                          'first_word_id': str(sentence.index(incorr_mwe[0])),\n",
    "                                          'second_word_id': str(sentence.index(incorr_mwe[1])),\n",
    "                                          'sentence': ' '.join(sentence)},\n",
    "                                          ignore_index=True)\n",
    "    if sent_idx % 10000 == 0 and sent_idx > 0:\n",
    "        print(f'{get_curr_time()} : Processed {sent_idx + 1} sentences')\n",
    "    sent_idx +=1\n",
    "\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12374827"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save dataframe to tsv\n",
    "measure_name = 'pmi'\n",
    "sent_df_out_filepath = os.path.join('..', 'storage', 'bnc', 'preprocessed_data', f'{measure_name}_incorrect_sentence_list.tsv')\n",
    "\n",
    "save_sentence_df(sent_df, sent_df_out_filepath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed0e62fcffcbe394f731cea46f2165e217aacd1d321b1b1cabb24840ab27ecb0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
