{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import BigramCollocationFinder, word_tokenize, pos_tag\n",
    "from nltk.corpus.reader import BNCCorpusReader\n",
    "from nltk.collocations import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/netherwulf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download perceptron PoS tagger\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read British National Corpus\n",
    "\n",
    "bnc_texts_dir = os.path.join('..', 'storage', 'bnc', 'raw_data', 'BNC', 'Texts')\n",
    "\n",
    "bnc_reader = BNCCorpusReader(root=bnc_texts_dir, fileids=r'[A-K]/\\w*/\\w*\\.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bigram association measures\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "measure_dict = {'pmi' : bigram_measures.pmi,\n",
    "                'dice' : bigram_measures.dice,\n",
    "                'chi2' : bigram_measures.chi_sq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize finder and preprocess BNC corpus\n",
    "\n",
    "# treat each sentence as another document (find MWEs inside sentences)\n",
    "finder = BigramCollocationFinder.from_documents(bnc_reader.sents())\n",
    "\n",
    "# treat whole corpus as a long sentence (find MWEs inside and BETWEEN sentences)\n",
    "# finder = BigramCollocationFinder.from_words(bnc_reader.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curr_time():\n",
    "    return f'{datetime.datetime.now().strftime(\"%H:%M:%S\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(nltk_pos_tag):\n",
    "    if nltk_pos_tag[0] == 'J':\n",
    "        return 'ADJ'\n",
    "        \n",
    "    if nltk_pos_tag[0] == 'N':\n",
    "        return 'NOUN'\n",
    "\n",
    "    if nltk_pos_tag[0] == 'V':\n",
    "        return 'VERB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check word correctness\n",
    "def check_word_correctness(word) -> bool:\n",
    "    if len(word) < 1:\n",
    "        return False\n",
    "    # print(f'word: {word}',\n",
    "    #       f'pos_tag: {pos_tag([word])}',\n",
    "    #       f'pos_tag([word])[0][1][0]: {pos_tag([word])[0][1][0]}',\n",
    "    #       sep='\\n')\n",
    "\n",
    "    if any(char in string.punctuation for char in word):\n",
    "        return False\n",
    "\n",
    "    if any(char in ['`', '~', '‘', '—', '\\''] for char in word):\n",
    "        return False\n",
    "\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return False\n",
    "\n",
    "    if word[0].isupper():\n",
    "        return False\n",
    "\n",
    "    if pos_tag([word])[0][1][0] not in ['J', 'N', 'V']:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of occurrences for MWE\n",
    "def get_mwe_freq(mwe, freq_list):\n",
    "    # mwe_tuple = [mwe_tuple for mwe_tuple in freq_list if mwe_tuple[0] == mwe]\n",
    "    return freq_list[mwe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of measures to the TSV file\n",
    "def save_mwe_list(mwe_list, measure_name, dataset_name, dataset_dir):\n",
    "    out_filepath = os.path.join(dataset_dir, f'{dataset_name}_{measure_name}_incorrect_mwe.tsv')\n",
    "    with open(out_filepath, 'w') as out_file:\n",
    "        out_file.write('\\t'.join(['first_word_tag', 'second_word_tag', 'first_word', 'second_word', 'measure_value', 'frequency']) + '\\n')\n",
    "\n",
    "        for mwe_tuple in mwe_list:\n",
    "            out_file.write('\\t'.join(mwe_tuple) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:20:20 : Generating incorrect MWEs list for pmi\n"
     ]
    }
   ],
   "source": [
    "# get cleaned list of MWEs for list of measures\n",
    "\n",
    "dataset_name = 'bnc'\n",
    "dataset_dir = os.path.join('..', 'storage', dataset_name, 'preprocessed_data')\n",
    "\n",
    "for measure_name in measure_dict.keys():\n",
    "    if measure_name != 'pmi':\n",
    "        continue\n",
    "\n",
    "    print(f'{get_curr_time()} : Generating incorrect MWEs list for {measure_name}')\n",
    "\n",
    "    # get list of MWEs with spoecified measure\n",
    "    desc_mwe_list = finder.score_ngrams(measure_dict[measure_name])\n",
    "\n",
    "    # get frequencies of MWE\n",
    "    freq_mwe_list = {k: v for k, v in finder.ngram_fd.items()}\n",
    "\n",
    "    # clean list of MWEs\n",
    "    desc_mwe_list_cleaned = [mwe_tuple for mwe_tuple in desc_mwe_list[::-1] if all([check_word_correctness(mwe_word) for mwe_word in mwe_tuple[0]]) and mwe_tuple[1] > 1][:100000]\n",
    "\n",
    "    # get list with MWE, measure value and frequency\n",
    "    mwe_with_freq = [[get_pos_tag(pos_tag([mwe_tuple[0][0]])[0][1]), get_pos_tag(pos_tag([mwe_tuple[0][1]])[0][1]), mwe_tuple[0][0], mwe_tuple[0][1], str(mwe_tuple[1]), str(get_mwe_freq(mwe_tuple[0], freq_mwe_list))] for mwe_tuple in desc_mwe_list_cleaned]\n",
    "\n",
    "    # save dataset\n",
    "    save_mwe_list(mwe_with_freq, measure_name, dataset_name, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:31:14 : Generating incorrect MWEs list for chi2\n"
     ]
    }
   ],
   "source": [
    "# get cleaned list of MWEs for single measure\n",
    "\n",
    "measure_name = 'chi2'\n",
    "\n",
    "dataset_name = 'bnc'\n",
    "\n",
    "dataset_dir = os.path.join('..', 'storage', dataset_name, 'preprocessed_data')\n",
    "\n",
    "print(f'{get_curr_time()} : Generating incorrect MWEs list for {measure_name}')\n",
    "\n",
    "# get list of MWEs with spoecified measure\n",
    "desc_mwe_list = finder.score_ngrams(measure_dict[measure_name])\n",
    "\n",
    "# get frequencies of MWE\n",
    "freq_mwe_list = {k: v for k, v in finder.ngram_fd.items()}\n",
    "\n",
    "# clean list of MWEs\n",
    "# desc_mwe_list_cleaned = [mwe_tuple for mwe_tuple in desc_mwe_list[::-1] if all([check_word_correctness(mwe_word) for mwe_word in mwe_tuple[0]]) and mwe_tuple[1] > 1]\n",
    "desc_mwe_list_cleaned = [mwe_tuple for mwe_tuple in desc_mwe_list[::-1] if all([check_word_correctness(mwe_word) for mwe_word in mwe_tuple[0]])]\n",
    "\n",
    "# get last values from the end of the 3rd quartile\n",
    "desc_mwe_list_cleaned = desc_mwe_list_cleaned[int(0.75 * len(desc_mwe_list_cleaned)) - 100000:int(0.75 * len(desc_mwe_list_cleaned))]\n",
    "\n",
    "# get list with MWE, measure value and frequency\n",
    "mwe_with_freq = [[get_pos_tag(pos_tag([mwe_tuple[0][0]])[0][1]), get_pos_tag(pos_tag([mwe_tuple[0][1]])[0][1]), mwe_tuple[0][0], mwe_tuple[0][1], str(mwe_tuple[1]), str(get_mwe_freq(mwe_tuple[0], freq_mwe_list))] for mwe_tuple in desc_mwe_list_cleaned]\n",
    "\n",
    "# save dataset\n",
    "save_mwe_list(mwe_with_freq, f'{measure_name}_end_of_3rd_quartile', dataset_name, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pos_tag(pos_tag(['banana'])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12374827"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc_mwe_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed0e62fcffcbe394f731cea46f2165e217aacd1d321b1b1cabb24840ab27ecb0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
